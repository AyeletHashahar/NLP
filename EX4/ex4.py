# -*- coding: utf-8 -*-
"""intro2nlp_ex4_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kGiHk1lcADLxRzts9QfqTnTSD9Z_TlkT

## IMPORTANT:

1.   You must use secure BGU connection (either locally from BGU or with a VPN)
2.   You cannot run the collab code, even with a VPN - it will be blocked by the server. You have to COPY the block below to a local .py (or a local notebook, but you should submit a .py file) and run it.
3. Update your datapath in data_path parameter.
4. You could experiment with other models, either locally installed or paid subscriptions. However, we cannot provide IT support for other models and platforms.
5. Finally, last but not least - don't wait for the last minute. The LLM servers do not support a very heavy throughput and response in a FIFO manner. If all of you access at the same time (e.g., in the last minute) you will have to wait or even get a time-out error.
"""

# Setting the IP of the server supporting the requiered model.

import requests,json, csv
from collections import Counter
import random
import re


# data_path points to the folder that holds the TSVs
data_path = 'data'

# file with the 30 evaluation tweets (already correct)
data_fn = 'echo_posts_2_labels_short.tsv'

# full file name of the 4 600-row corpus        
long_fn = 'echo_posts_2_labels_long.tsv'


SERVER_IP = "132.72.64.37" # faster responses but smaller models
#SERVER_IP = "132.72.65.18" # use this for the larger models
model_name = 'gemma3:27b'


#helper functions

def load_data(full_path, has_header = True):
    """
    Laods parses and return files in the format of echo_posts_2_labels_short.tsv
    """
    with open(full_path, 'r', encoding='utf-8') as fin:
        tsv_file = csv.reader(fin, delimiter='\t')
        dataset = [e for e in tsv_file]

    if has_header:
        return dataset[1:]
    return dataset


def prepare_request(p: str,model_name : str = 'gemma3:27b'):
    data = {'model': model_name, 'prompt': p}
    return data


def query_model(data, server_ip = SERVER_IP):
    response = requests.post(f"http://{server_ip}:11434/api/generate", json=data)
    #print(response.text)
    lines = response.text.strip().split('\n')
    res = ''.join(json.loads(line)['response'] for line in lines if line.strip())
    return res


def getPrompt1_1(t):
    """
    Args:
        t: The text to be classified.

    Returns:
        The prompt as specified.
    """
    return f"Is the following text considered hate speech? The text is: [{t}]"


def getPrompt1_2(t):
    """
    Args:
        t: The text to be classified.

    Returns:
        The prompt as specified.
    """
    return f"Is the following text considered hate speech? The text is: [{t}]. Answer only with 'yes' or 'no'."

######################################
#     Your work starts here
######################################

# helper – pick 5 seed examples

def select_five_examples(long_path, short_path):
    long_ds  = load_data(long_path)           # all 4 600 rows
    short_ds = load_data(short_path)          # the 30-row eval set

    short_texts = {t for t, _ in short_ds}    # for fast lookup
    pool = [row for row in long_ds if row[0] not in short_texts]

    # keep 3 hateful (label '1') and 2 non-hateful (label '0')
    hateful     = [row for row in pool if row[1] == '1']
    non_hateful = [row for row in pool if row[1] == '0']

    examples = random.sample(hateful, 3) + random.sample(non_hateful, 2)
    random.shuffle(examples)                  # avoid obvious ordering

    # convert → ("tweet_text", "yes" | "no")
    return [(t, 'yes' if lbl == '1' else 'no') for t, lbl in examples]

random.seed(42)   
FEW_SHOT_EXAMPLES = select_five_examples(
    f"{data_path}/{long_fn}",        # <-- long file, not 30-row file
    f"{data_path}/{data_fn}"
)

def getPrompt2(t):
    """
    Args:
        t: The text to be classified.

    Returns:
        The prompt as specified.
    """
    header = ("Below are examples of tweets with their labels "
              "('yes' = hate speech, 'no' = not hate speech).\n\n")

    shots = "\n".join(
        f'Tweet: "{ex_t}"\nLabel: {ex_lbl}.'
        for ex_t, ex_lbl in FEW_SHOT_EXAMPLES
    )

    question = (f'\n\nNow classify the next tweet in the *same* way.\n'
                f'Tweet: "{t}"\nLabel:')

    footer = " Answer with exactly one word: 'yes' or 'no'."
    return header + shots + question + footer

def getPrompt3(t):
    """
    Args:
        t: The text to be classified.

    Returns:
        The prompt as specified.
    """
    header = ("Below are labeled examples: "
              "'yes' = hateful, 'no' = not hateful.\n\n")

    shots = "\n".join(
        f"Tweet: \"{ex_t}\"\nLabel: {ex_lbl}."
        for ex_t, ex_lbl in FEW_SHOT_EXAMPLES
    )

    question = (f"\n\nNow classify the next tweet.\n"
                f"Tweet: \"{t}\"\nLabel:")

    footer = " Answer with exactly one word: 'yes' or 'no'."

    return header + shots + question + footer


def who_am_i():  # this is not a class method
    """Returns a list of dictionaries, each dictionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return [{'name': 'Ayelet Hashahar Cohen', 'id': '206533895', 'email': 'ayelethc@post.bgu.ac.il'}]

######################################
#     Run tasks 
######################################

# -----------------------# Task 1 --------------------------------
yes_no_re = re.compile(r"\b(yes|no)\b", re.I)

def extract_yes_no(text: str) -> str:
    """
    Return 'yes' if the first standalone token is 'yes',
    otherwise return 'no'.  Case-insensitive.  Works even if the
    model begins with filler like 'Okay,' or 'Let's see'.
    """
    m = yes_no_re.search(text)
    return "yes" if m and m.group(1).lower() == "yes" else "no"


def run_task1():
    short_ds = load_data(f"{data_path}/{data_fn}")          # 30 rows
    out_path = "task1.1.tsv"

    with open(out_path, "w", encoding="utf-8") as fout:
        for tweet, _ in short_ds:
            prompt = getPrompt1_1(tweet)
            raw    = query_model(prepare_request(prompt))

            label = extract_yes_no(raw)                       # ← NEW

            flat  = " ".join(raw.replace("\t", " ").split())  # keep one line
            fout.write(f"{flat}\t{label}\n")


    print(f"{out_path} written with {len(short_ds)} rows.")


    # ---- 1.2  forced yes/no  ----------------------------------------------
    with open("task1.2.tsv", "w", encoding="utf-8") as fout2:
        for text, _ in short_ds:
            prompt = getPrompt1_2(text)
            raw = query_model(prepare_request(prompt)).strip().lower()

            # keep only the *first* word the model outputs
            first_token = raw.split()[0].rstrip('.,;:!?') if raw else ""
            label = "yes" if first_token.startswith("y") else "no"
            fout2.write(f"{label}\n")

# -----------Task2------------------------------------------------
def run_task2():
    short_ds = load_data(f"{data_path}/{data_fn}")   # 30 eval tweets
    out_path = "task2.tsv"

    with open(out_path, "w", encoding="utf-8") as fout:
        for text, _ in short_ds:
            prompt = getPrompt2(text)
            raw    = query_model(prepare_request(prompt)).strip().lower()
            label  = "yes" if raw.split()[0].rstrip('.,;:!?').startswith("y") else "no"
            fout.write(f"{label}\n")

    print(f"Saved predictions to {out_path}")


# -----------------------# Task 3 --------------------------------

def majority_vote(labels):
    """Return 'yes', 'no', or 'tie' (rare)."""
    c = Counter(labels)
    if c["yes"] > c["no"]:
        return "yes"
    if c["no"] > c["yes"]:
        return "no"
    return "tie"

def run_task3():
        # ---------- load the short dataset (30 rows) ----------
    full_short_ds = load_data(f"{data_path}/{data_fn}")      # 30
    short_ds      = full_short_ds[:20]                       # first 20

    # ---------- build label dictionaries ----------
    task1_1_labels = [
    line.rsplit("\t", 1)[-1].strip()            # → 'yes' or 'no'
    for line in open("task1.1.tsv", encoding="utf-8")
    ]

    # Map each tweet (from full_short_ds) to its label by position
    task1_1_dict = {
        txt: lab
        for (txt, _), lab in zip(full_short_ds, task1_1_labels)
    }

    # task1.2 and task2 have only labels → preserve order
    task1_2_labels = [line.strip() for line in
                      open("task1.2.tsv", encoding="utf-8")]
    task2_labels   = [line.strip() for line in
                      open("task2.tsv",   encoding="utf-8")]

    task1_2_dict = {txt: lab for (txt, _), lab in
                    zip(full_short_ds, task1_2_labels)}
    task2_dict   = {txt: lab for (txt, _), lab in
                    zip(full_short_ds, task2_labels)}
    
    task3_rows = []

    for text, gold in short_ds:
        runs = []
        for _ in range(5):
            prompt   = getPrompt3(text)
            raw      = query_model(prepare_request(prompt)).strip().lower()
            label    = "yes" if raw.split()[0].rstrip('.,;:!?').startswith("y") else "no"
            runs.append(label)

        maj = majority_vote(runs)

        task3_rows.append({
            "text": text,
            "run_1": runs[0], "run_2": runs[1], "run_3": runs[2],
            "run_4": runs[3], "run_5": runs[4],
            "majority_prediction": maj,
            # you have these cached from previous tasks:
            "prediction_1.1": task1_1_dict[text],
            "prediction_1.2": task1_2_dict[text],
            "prediction_2":   task2_dict[text],
            "true_label": 'yes' if gold == '1' else 'no'
        })

    # --- write TSV ---
    with open("task3.tsv", "w", encoding="utf-8") as fout:
        header_cols = ["text", "run_1", "run_2", "run_3", "run_4", "run_5",
                       "majority_prediction",
                       "prediction_1.1", "prediction_1.2", "prediction_2",
                       "true_label"]
        fout.write("\t".join(header_cols) + "\n")
        for row in task3_rows:
            fout.write("\t".join(row[col] for col in header_cols) + "\n")

    print("task3.tsv written with", len(task3_rows), "rows.")




if __name__ == "__main__":
    # Uncomment the following line to run the task1 driver
    run_task1()
    # Uncomment the following line to run the task2 driver
    run_task2()
    # Uncomment the following line to run the task3 driver
    run_task3()

    # #example1: this is how you query the LLM
    # p = 'why is the sky blue?'
    # data = prepare_request(p)
    # res = query_model(data)
    # print(res)


    # #example2: this is how you query the LLM with the prompt defined in task1.2
    # p = getPrompt1_2("In a hole in the ground there lived a hobbit")
    # data = prepare_request(p)
    # res = query_model(data)
    # print(res)

    # p = getPrompt1_2("I hate all Smurfs and will destroy their village")
    # data = prepare_request(p)
    # res = query_model(data)
    # print(res)

    # p = getPrompt1_2("I hate all mexicans and will destroy their village")
    # data = prepare_request(p)
    # res = query_model(data)
    # print(res)